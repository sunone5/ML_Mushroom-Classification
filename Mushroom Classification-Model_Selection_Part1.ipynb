{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape_b</th>\n",
       "      <th>cap-shape_c</th>\n",
       "      <th>cap-shape_k</th>\n",
       "      <th>cap-shape_s</th>\n",
       "      <th>cap-shape_x</th>\n",
       "      <th>cap-surface_f</th>\n",
       "      <th>cap-surface_g</th>\n",
       "      <th>cap-surface_s</th>\n",
       "      <th>cap-surface_y</th>\n",
       "      <th>cap-color_b</th>\n",
       "      <th>cap-color_c</th>\n",
       "      <th>cap-color_e</th>\n",
       "      <th>cap-color_g</th>\n",
       "      <th>cap-color_n</th>\n",
       "      <th>cap-color_p</th>\n",
       "      <th>cap-color_r</th>\n",
       "      <th>cap-color_u</th>\n",
       "      <th>cap-color_w</th>\n",
       "      <th>cap-color_y</th>\n",
       "      <th>bruises_f</th>\n",
       "      <th>bruises_t</th>\n",
       "      <th>odor_a</th>\n",
       "      <th>odor_c</th>\n",
       "      <th>odor_f</th>\n",
       "      <th>odor_l</th>\n",
       "      <th>odor_m</th>\n",
       "      <th>odor_n</th>\n",
       "      <th>odor_p</th>\n",
       "      <th>odor_s</th>\n",
       "      <th>odor_y</th>\n",
       "      <th>gill-attachment_a</th>\n",
       "      <th>gill-attachment_f</th>\n",
       "      <th>gill-spacing_c</th>\n",
       "      <th>gill-spacing_w</th>\n",
       "      <th>gill-size_b</th>\n",
       "      <th>gill-size_n</th>\n",
       "      <th>gill-color_b</th>\n",
       "      <th>gill-color_e</th>\n",
       "      <th>gill-color_g</th>\n",
       "      <th>gill-color_h</th>\n",
       "      <th>gill-color_k</th>\n",
       "      <th>gill-color_n</th>\n",
       "      <th>gill-color_o</th>\n",
       "      <th>gill-color_p</th>\n",
       "      <th>gill-color_r</th>\n",
       "      <th>gill-color_u</th>\n",
       "      <th>gill-color_w</th>\n",
       "      <th>gill-color_y</th>\n",
       "      <th>stalk-shape_e</th>\n",
       "      <th>stalk-shape_t</th>\n",
       "      <th>stalk-root_c</th>\n",
       "      <th>stalk-root_e</th>\n",
       "      <th>stalk-root_r</th>\n",
       "      <th>stalk-surface-above-ring_f</th>\n",
       "      <th>stalk-surface-above-ring_k</th>\n",
       "      <th>stalk-surface-above-ring_s</th>\n",
       "      <th>stalk-surface-below-ring_f</th>\n",
       "      <th>stalk-surface-below-ring_k</th>\n",
       "      <th>stalk-surface-below-ring_s</th>\n",
       "      <th>stalk-surface-below-ring_y</th>\n",
       "      <th>stalk-color-above-ring_b</th>\n",
       "      <th>stalk-color-above-ring_c</th>\n",
       "      <th>stalk-color-above-ring_e</th>\n",
       "      <th>stalk-color-above-ring_g</th>\n",
       "      <th>stalk-color-above-ring_n</th>\n",
       "      <th>stalk-color-above-ring_o</th>\n",
       "      <th>stalk-color-above-ring_p</th>\n",
       "      <th>stalk-color-above-ring_w</th>\n",
       "      <th>stalk-color-above-ring_y</th>\n",
       "      <th>stalk-color-below-ring_b</th>\n",
       "      <th>stalk-color-below-ring_c</th>\n",
       "      <th>stalk-color-below-ring_e</th>\n",
       "      <th>stalk-color-below-ring_g</th>\n",
       "      <th>stalk-color-below-ring_n</th>\n",
       "      <th>stalk-color-below-ring_o</th>\n",
       "      <th>stalk-color-below-ring_p</th>\n",
       "      <th>stalk-color-below-ring_w</th>\n",
       "      <th>stalk-color-below-ring_y</th>\n",
       "      <th>veil-color_n</th>\n",
       "      <th>veil-color_o</th>\n",
       "      <th>veil-color_w</th>\n",
       "      <th>veil-color_y</th>\n",
       "      <th>ring-number_n</th>\n",
       "      <th>ring-number_o</th>\n",
       "      <th>ring-number_t</th>\n",
       "      <th>ring-type_e</th>\n",
       "      <th>ring-type_f</th>\n",
       "      <th>ring-type_l</th>\n",
       "      <th>ring-type_n</th>\n",
       "      <th>ring-type_p</th>\n",
       "      <th>spore-print-color_b</th>\n",
       "      <th>spore-print-color_h</th>\n",
       "      <th>spore-print-color_k</th>\n",
       "      <th>spore-print-color_n</th>\n",
       "      <th>spore-print-color_o</th>\n",
       "      <th>spore-print-color_r</th>\n",
       "      <th>spore-print-color_u</th>\n",
       "      <th>spore-print-color_w</th>\n",
       "      <th>spore-print-color_y</th>\n",
       "      <th>population_a</th>\n",
       "      <th>population_c</th>\n",
       "      <th>population_n</th>\n",
       "      <th>population_s</th>\n",
       "      <th>population_v</th>\n",
       "      <th>population_y</th>\n",
       "      <th>habitat_d</th>\n",
       "      <th>habitat_g</th>\n",
       "      <th>habitat_l</th>\n",
       "      <th>habitat_m</th>\n",
       "      <th>habitat_p</th>\n",
       "      <th>habitat_u</th>\n",
       "      <th>habitat_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  cap-shape_b  cap-shape_c  cap-shape_k  cap-shape_s  cap-shape_x  cap-surface_f  cap-surface_g  cap-surface_s  cap-surface_y  cap-color_b  cap-color_c  cap-color_e  cap-color_g  cap-color_n  cap-color_p  cap-color_r  cap-color_u  cap-color_w  cap-color_y  bruises_f  bruises_t  odor_a  odor_c  odor_f  odor_l  odor_m  odor_n  odor_p  odor_s  odor_y  gill-attachment_a  gill-attachment_f  gill-spacing_c  gill-spacing_w  gill-size_b  gill-size_n  gill-color_b  gill-color_e  gill-color_g  gill-color_h  gill-color_k  gill-color_n  gill-color_o  gill-color_p  gill-color_r  gill-color_u  gill-color_w  gill-color_y  stalk-shape_e  stalk-shape_t  stalk-root_c  stalk-root_e  stalk-root_r  stalk-surface-above-ring_f  stalk-surface-above-ring_k  stalk-surface-above-ring_s  stalk-surface-below-ring_f  stalk-surface-below-ring_k  stalk-surface-below-ring_s  stalk-surface-below-ring_y  stalk-color-above-ring_b  stalk-color-above-ring_c  stalk-color-above-ring_e  stalk-color-above-ring_g  \\\n",
       "0      0            0            0            0            0            0              1              0              0              0            0            0            0            0            1            0            0            0            0            0          0          1       0       0       0       0       0       1       0       0       0                  0                  1               1               0            1            0             0             0             0             0             0             1             0             0             0             0             0             0              0              1             0             0             0                           0                           0                           1                           0                           0                           1                           0                         0                         0                         0                         0   \n",
       "1      0            0            0            0            0            1              0              0              1              0            0            0            0            0            0            0            0            0            1            0          0          1       1       0       0       0       0       0       0       0       0                  0                  1               1               0            1            0             0             0             1             0             0             0             0             0             0             0             0             0              1              0             1             0             0                           0                           0                           1                           0                           0                           1                           0                         0                         0                         0                         0   \n",
       "2      0            0            0            0            0            1              1              0              0              0            0            0            0            0            0            0            0            0            1            0          1          0       0       0       0       0       0       1       0       0       0                  0                  1               0               1            1            0             0             0             0             1             0             0             0             0             0             0             0             0              0              1             0             1             0                           1                           0                           0                           0                           0                           1                           0                         0                         0                         0                         0   \n",
       "3      0            0            0            0            0            0              0              0              0              1            0            0            0            0            1            0            0            0            0            0          0          1       0       0       0       1       0       0       0       0       0                  0                  1               1               0            1            0             0             0             0             0             0             1             0             0             0             0             0             0              1              0             0             0             1                           0                           0                           1                           0                           0                           0                           1                         0                         0                         0                         0   \n",
       "4      0            1            0            0            0            0              0              0              1              0            0            0            0            0            0            0            0            0            1            0          0          1       1       0       0       0       0       0       0       0       0                  0                  1               1               0            1            0             0             0             0             0             0             1             0             0             0             0             0             0              1              0             1             0             0                           0                           0                           1                           0                           0                           1                           0                         0                         0                         0                         0   \n",
       "\n",
       "   stalk-color-above-ring_n  stalk-color-above-ring_o  stalk-color-above-ring_p  stalk-color-above-ring_w  stalk-color-above-ring_y  stalk-color-below-ring_b  stalk-color-below-ring_c  stalk-color-below-ring_e  stalk-color-below-ring_g  stalk-color-below-ring_n  stalk-color-below-ring_o  stalk-color-below-ring_p  stalk-color-below-ring_w  stalk-color-below-ring_y  veil-color_n  veil-color_o  veil-color_w  veil-color_y  ring-number_n  ring-number_o  ring-number_t  ring-type_e  ring-type_f  ring-type_l  ring-type_n  ring-type_p  spore-print-color_b  spore-print-color_h  spore-print-color_k  spore-print-color_n  spore-print-color_o  spore-print-color_r  spore-print-color_u  spore-print-color_w  spore-print-color_y  population_a  population_c  population_n  population_s  population_v  population_y  habitat_d  habitat_g  habitat_l  habitat_m  habitat_p  habitat_u  habitat_w  \n",
       "0                         0                         0                         0                         1                         0                         0                         0                         0                         0                         0                         0                         0                         1                         0             0             0             1             0              0              1              0            0            0            0            0            1                    0                    0                    0                    1                    0                    0                    0                    0                    0             0             0             0             0             0             1          1          0          0          0          0          0          0  \n",
       "1                         0                         0                         0                         1                         0                         0                         0                         0                         0                         0                         0                         0                         1                         0             0             0             1             0              0              1              0            0            0            0            0            1                    0                    0                    1                    0                    0                    0                    0                    0                    0             0             0             1             0             0             0          0          1          0          0          0          0          0  \n",
       "2                         0                         0                         0                         1                         0                         0                         0                         0                         0                         0                         0                         0                         1                         0             0             0             1             0              0              1              0            1            0            0            0            0                    0                    0                    1                    0                    0                    0                    0                    0                    0             0             0             0             1             0             0          0          1          0          0          0          0          0  \n",
       "3                         0                         0                         0                         1                         0                         0                         0                         0                         0                         0                         0                         0                         1                         0             0             0             1             0              0              1              0            0            0            0            0            1                    0                    0                    1                    0                    0                    0                    0                    0                    0             0             0             0             0             0             1          0          0          0          0          1          0          0  \n",
       "4                         0                         0                         0                         1                         0                         0                         0                         0                         0                         0                         0                         0                         1                         0             0             0             1             0              0              1              0            0            0            0            0            1                    0                    0                    0                    1                    0                    0                    0                    0                    0             0             0             1             0             0             0          0          1          0          0          0          0          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./output_data/engineered_data.csv').drop('Unnamed: 0',axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Correlation with target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>odor_n</td>\n",
       "      <td>-0.785557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>odor_f</td>\n",
       "      <td>0.623842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stalk-surface-above-ring_k</td>\n",
       "      <td>0.587658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stalk-surface-below-ring_k</td>\n",
       "      <td>0.573524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ring-type_p</td>\n",
       "      <td>-0.540469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gill-size_n</td>\n",
       "      <td>0.540024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gill-size_b</td>\n",
       "      <td>-0.540024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gill-color_b</td>\n",
       "      <td>0.538808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bruises_f</td>\n",
       "      <td>0.501530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bruises_t</td>\n",
       "      <td>-0.501530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stalk-surface-above-ring_s</td>\n",
       "      <td>-0.491314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spore-print-color_h</td>\n",
       "      <td>0.490229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ring-type_l</td>\n",
       "      <td>0.451619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>population_v</td>\n",
       "      <td>0.443722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stalk-surface-below-ring_s</td>\n",
       "      <td>-0.425444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spore-print-color_n</td>\n",
       "      <td>-0.416645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spore-print-color_k</td>\n",
       "      <td>-0.396832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spore-print-color_w</td>\n",
       "      <td>0.357384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gill-spacing_w</td>\n",
       "      <td>-0.348387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gill-spacing_c</td>\n",
       "      <td>0.348387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>habitat_p</td>\n",
       "      <td>0.323346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gill-color_n</td>\n",
       "      <td>-0.288943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>odor_y</td>\n",
       "      <td>0.286360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>odor_s</td>\n",
       "      <td>0.286360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stalk-color-below-ring_g</td>\n",
       "      <td>-0.266489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stalk-color-above-ring_g</td>\n",
       "      <td>-0.266489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stalk-color-below-ring_b</td>\n",
       "      <td>0.245662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stalk-color-above-ring_b</td>\n",
       "      <td>0.245662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stalk-color-above-ring_n</td>\n",
       "      <td>0.233164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gill-color_w</td>\n",
       "      <td>-0.231316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>stalk-color-below-ring_p</td>\n",
       "      <td>0.230277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>stalk-color-above-ring_p</td>\n",
       "      <td>0.230277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ring-type_e</td>\n",
       "      <td>0.223286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>population_n</td>\n",
       "      <td>-0.219529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>odor_a</td>\n",
       "      <td>-0.219529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>odor_l</td>\n",
       "      <td>-0.219529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>stalk-root_c</td>\n",
       "      <td>-0.218548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>stalk-color-above-ring_w</td>\n",
       "      <td>-0.217740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>population_a</td>\n",
       "      <td>-0.214871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>stalk-color-below-ring_w</td>\n",
       "      <td>-0.214112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ring-number_t</td>\n",
       "      <td>-0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>stalk-color-below-ring_n</td>\n",
       "      <td>0.203966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>stalk-root_e</td>\n",
       "      <td>-0.202839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>cap-surface_f</td>\n",
       "      <td>-0.195415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>gill-color_u</td>\n",
       "      <td>-0.195359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>odor_p</td>\n",
       "      <td>0.186984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>cap-shape_b</td>\n",
       "      <td>-0.182567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ring-number_o</td>\n",
       "      <td>0.182101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>habitat_g</td>\n",
       "      <td>-0.165004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>cap-shape_k</td>\n",
       "      <td>0.163565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>odor_c</td>\n",
       "      <td>0.161278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>population_s</td>\n",
       "      <td>-0.159572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>habitat_l</td>\n",
       "      <td>0.155150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>gill-color_h</td>\n",
       "      <td>0.150694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>stalk-root_r</td>\n",
       "      <td>-0.150087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>stalk-color-below-ring_o</td>\n",
       "      <td>-0.150087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>stalk-color-above-ring_o</td>\n",
       "      <td>-0.150087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>habitat_w</td>\n",
       "      <td>-0.150087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>gill-color_k</td>\n",
       "      <td>-0.149641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>veil-color_w</td>\n",
       "      <td>0.140541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>habitat_m</td>\n",
       "      <td>-0.138627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>population_c</td>\n",
       "      <td>-0.137645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>stalk-surface-below-ring_f</td>\n",
       "      <td>-0.136782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>cap-color_w</td>\n",
       "      <td>-0.133683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>gill-attachment_a</td>\n",
       "      <td>-0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>gill-attachment_f</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>habitat_d</td>\n",
       "      <td>-0.126123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>gill-color_g</td>\n",
       "      <td>0.120285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>stalk-surface-above-ring_f</td>\n",
       "      <td>-0.119503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>cap-color_y</td>\n",
       "      <td>0.113014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>habitat_u</td>\n",
       "      <td>0.112078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>population_y</td>\n",
       "      <td>-0.107055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>stalk-color-below-ring_e</td>\n",
       "      <td>-0.105491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>veil-color_o</td>\n",
       "      <td>-0.105491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>veil-color_n</td>\n",
       "      <td>-0.105491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>gill-color_e</td>\n",
       "      <td>-0.105491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>stalk-color-above-ring_e</td>\n",
       "      <td>-0.105491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>stalk-shape_e</td>\n",
       "      <td>0.102019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>stalk-shape_t</td>\n",
       "      <td>-0.102019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>spore-print-color_r</td>\n",
       "      <td>0.098024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>cap-color_e</td>\n",
       "      <td>0.097112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>cap-surface_s</td>\n",
       "      <td>0.095454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>cap-surface_y</td>\n",
       "      <td>0.088677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>gill-color_o</td>\n",
       "      <td>-0.085962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>stalk-surface-below-ring_y</td>\n",
       "      <td>-0.081674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>spore-print-color_b</td>\n",
       "      <td>-0.074371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ring-type_f</td>\n",
       "      <td>-0.074371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>spore-print-color_y</td>\n",
       "      <td>-0.074371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>spore-print-color_o</td>\n",
       "      <td>-0.074371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>spore-print-color_u</td>\n",
       "      <td>-0.074371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>odor_m</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ring-number_n</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>stalk-color-above-ring_c</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>ring-type_n</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>stalk-color-below-ring_c</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>cap-color_b</td>\n",
       "      <td>0.067544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>cap-shape_s</td>\n",
       "      <td>-0.060664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>gill-color_r</td>\n",
       "      <td>0.056426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>stalk-color-below-ring_y</td>\n",
       "      <td>0.056426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>gill-color_p</td>\n",
       "      <td>-0.050380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>gill-color_y</td>\n",
       "      <td>-0.046828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>cap-color_g</td>\n",
       "      <td>-0.046456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>cap-color_n</td>\n",
       "      <td>-0.044360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>cap-color_u</td>\n",
       "      <td>-0.042854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>cap-color_r</td>\n",
       "      <td>-0.042854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cap-color_p</td>\n",
       "      <td>0.034702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>stalk-color-above-ring_y</td>\n",
       "      <td>0.032545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>veil-color_y</td>\n",
       "      <td>0.032545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cap-color_c</td>\n",
       "      <td>-0.030910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cap-shape_x</td>\n",
       "      <td>-0.026886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cap-surface_g</td>\n",
       "      <td>0.023007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cap-shape_c</td>\n",
       "      <td>0.023007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature  Correlation with target\n",
       "0                         class                 1.000000\n",
       "1                        odor_n                -0.785557\n",
       "2                        odor_f                 0.623842\n",
       "3    stalk-surface-above-ring_k                 0.587658\n",
       "4    stalk-surface-below-ring_k                 0.573524\n",
       "5                   ring-type_p                -0.540469\n",
       "6                   gill-size_n                 0.540024\n",
       "7                   gill-size_b                -0.540024\n",
       "8                  gill-color_b                 0.538808\n",
       "9                     bruises_f                 0.501530\n",
       "10                    bruises_t                -0.501530\n",
       "11   stalk-surface-above-ring_s                -0.491314\n",
       "12          spore-print-color_h                 0.490229\n",
       "13                  ring-type_l                 0.451619\n",
       "14                 population_v                 0.443722\n",
       "15   stalk-surface-below-ring_s                -0.425444\n",
       "16          spore-print-color_n                -0.416645\n",
       "17          spore-print-color_k                -0.396832\n",
       "18          spore-print-color_w                 0.357384\n",
       "19               gill-spacing_w                -0.348387\n",
       "20               gill-spacing_c                 0.348387\n",
       "21                    habitat_p                 0.323346\n",
       "22                 gill-color_n                -0.288943\n",
       "23                       odor_y                 0.286360\n",
       "24                       odor_s                 0.286360\n",
       "25     stalk-color-below-ring_g                -0.266489\n",
       "26     stalk-color-above-ring_g                -0.266489\n",
       "27     stalk-color-below-ring_b                 0.245662\n",
       "28     stalk-color-above-ring_b                 0.245662\n",
       "29     stalk-color-above-ring_n                 0.233164\n",
       "30                 gill-color_w                -0.231316\n",
       "31     stalk-color-below-ring_p                 0.230277\n",
       "32     stalk-color-above-ring_p                 0.230277\n",
       "33                  ring-type_e                 0.223286\n",
       "34                 population_n                -0.219529\n",
       "35                       odor_a                -0.219529\n",
       "36                       odor_l                -0.219529\n",
       "37                 stalk-root_c                -0.218548\n",
       "38     stalk-color-above-ring_w                -0.217740\n",
       "39                 population_a                -0.214871\n",
       "40     stalk-color-below-ring_w                -0.214112\n",
       "41                ring-number_t                -0.204600\n",
       "42     stalk-color-below-ring_n                 0.203966\n",
       "43                 stalk-root_e                -0.202839\n",
       "44                cap-surface_f                -0.195415\n",
       "45                 gill-color_u                -0.195359\n",
       "46                       odor_p                 0.186984\n",
       "47                  cap-shape_b                -0.182567\n",
       "48                ring-number_o                 0.182101\n",
       "49                    habitat_g                -0.165004\n",
       "50                  cap-shape_k                 0.163565\n",
       "51                       odor_c                 0.161278\n",
       "52                 population_s                -0.159572\n",
       "53                    habitat_l                 0.155150\n",
       "54                 gill-color_h                 0.150694\n",
       "55                 stalk-root_r                -0.150087\n",
       "56     stalk-color-below-ring_o                -0.150087\n",
       "57     stalk-color-above-ring_o                -0.150087\n",
       "58                    habitat_w                -0.150087\n",
       "59                 gill-color_k                -0.149641\n",
       "60                 veil-color_w                 0.140541\n",
       "61                    habitat_m                -0.138627\n",
       "62                 population_c                -0.137645\n",
       "63   stalk-surface-below-ring_f                -0.136782\n",
       "64                  cap-color_w                -0.133683\n",
       "65            gill-attachment_a                -0.129200\n",
       "66            gill-attachment_f                 0.129200\n",
       "67                    habitat_d                -0.126123\n",
       "68                 gill-color_g                 0.120285\n",
       "69   stalk-surface-above-ring_f                -0.119503\n",
       "70                  cap-color_y                 0.113014\n",
       "71                    habitat_u                 0.112078\n",
       "72                 population_y                -0.107055\n",
       "73     stalk-color-below-ring_e                -0.105491\n",
       "74                 veil-color_o                -0.105491\n",
       "75                 veil-color_n                -0.105491\n",
       "76                 gill-color_e                -0.105491\n",
       "77     stalk-color-above-ring_e                -0.105491\n",
       "78                stalk-shape_e                 0.102019\n",
       "79                stalk-shape_t                -0.102019\n",
       "80          spore-print-color_r                 0.098024\n",
       "81                  cap-color_e                 0.097112\n",
       "82                cap-surface_s                 0.095454\n",
       "83                cap-surface_y                 0.088677\n",
       "84                 gill-color_o                -0.085962\n",
       "85   stalk-surface-below-ring_y                -0.081674\n",
       "86          spore-print-color_b                -0.074371\n",
       "87                  ring-type_f                -0.074371\n",
       "88          spore-print-color_y                -0.074371\n",
       "89          spore-print-color_o                -0.074371\n",
       "90          spore-print-color_u                -0.074371\n",
       "91                       odor_m                 0.069159\n",
       "92                ring-number_n                 0.069159\n",
       "93     stalk-color-above-ring_c                 0.069159\n",
       "94                  ring-type_n                 0.069159\n",
       "95     stalk-color-below-ring_c                 0.069159\n",
       "96                  cap-color_b                 0.067544\n",
       "97                  cap-shape_s                -0.060664\n",
       "98                 gill-color_r                 0.056426\n",
       "99     stalk-color-below-ring_y                 0.056426\n",
       "100                gill-color_p                -0.050380\n",
       "101                gill-color_y                -0.046828\n",
       "102                 cap-color_g                -0.046456\n",
       "103                 cap-color_n                -0.044360\n",
       "104                 cap-color_u                -0.042854\n",
       "105                 cap-color_r                -0.042854\n",
       "106                 cap-color_p                 0.034702\n",
       "107    stalk-color-above-ring_y                 0.032545\n",
       "108                veil-color_y                 0.032545\n",
       "109                 cap-color_c                -0.030910\n",
       "110                 cap-shape_x                -0.026886\n",
       "111               cap-surface_g                 0.023007\n",
       "112                 cap-shape_c                 0.023007"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./output_data/engineered_data.csv').drop('Unnamed: 0',axis=1)\n",
    "corr = data.corr()['class']\n",
    "corr = pd.DataFrame(corr).reset_index().rename(columns={'index':'Feature','class':'Correlation with target'})\n",
    "corr['absolute'] = corr['Correlation with target'].apply(lambda x:np.abs(x))\n",
    "corr = corr.sort_values(by='absolute',ascending=False).reset_index().drop('index',axis=1).drop('absolute',axis=1)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_corr = df.corr()\n",
    "plt.figure(figsize=(12,6))\n",
    "x=0.348387\n",
    "bins=int(np.sqrt(len(df_corr['class'].unique())))\n",
    "plt.hist(df_corr['class'],bins=bins)\n",
    "plt.title('Histogram of correlations coefficients \\n between the category (target) and each feature',fontsize=20)\n",
    "plt.axvline(x=x,color='red')\n",
    "plt.axvline(x=-x,color='red')\n",
    "text = \"Features with an absolute\\n valued coefficient \\n greater than 0.34847 were key \\n (outside these red lines)\"\n",
    "plt.text(0.37,15,text,fontsize=15)\n",
    "plt.xlabel(\"correlation coefficient values for each feature\")\n",
    "plt.ylabel(\"frequency count\")\n",
    "plt.savefig('hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "tables = []\n",
    "thresholds = [0,0.15,0.20,0.25,0.3,0.35,0.4,0.45,0.50,0.51,.54,0.5401,0.54002,0.540025,0.55,0.6,0.65,0.7]\n",
    "print(len(thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>time</th>\n",
       "      <th>threshold</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105523</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025995</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.573260</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.365889</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy score  F1 score       time  threshold  features\n",
       "0  Logistic Regression             1.0       1.0   0.105523          0       112\n",
       "1        Decision Tree             1.0       1.0   0.025995          0       112\n",
       "2        Random Forest             1.0       1.0   0.148302          0       112\n",
       "3  K-Nearest Neighbors             1.0       1.0   2.573260          0       112\n",
       "4           Linear SVM             1.0       1.0  10.365889          0       112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,roc_curve,auc\n",
    "\n",
    "thresholds = [0,0.15,0.20,0.25,0.3,0.35,0.4,0.45,0.50,0.51,.54,0.5401,0.54002,0.540025,0.55,0.6,0.65,0.7]\n",
    "\n",
    "threshold = thresholds[0]\n",
    "features = list(df_corr[np.abs(df_corr['class'])>threshold]['class'].index)\n",
    "X = df[features].drop('class',axis=1)\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "keys = []\n",
    "scores = []\n",
    "times = []\n",
    "f1_scores = []\n",
    "models = {'Logistic Regression': LogisticRegression(class_weight='balanced'), 'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "          'Random Forest': RandomForestClassifier(n_estimators=30,class_weight='balanced'), \n",
    "          'K-Nearest Neighbors':KNeighborsClassifier(n_neighbors=1),\n",
    "            'Linear SVM':SVC(kernel='rbf', gamma=.10, C=1.0,probability=True)}\n",
    "\n",
    "for k,v in models.items():\n",
    "    model = v\n",
    "    t0=time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time()-t0\n",
    "    t1 = time.time()\n",
    "    pred = model.predict(X_test)\n",
    "    predict_time = time.time()-t1\n",
    "    Time_total = train_time+predict_time\n",
    "    times.append(Time_total)\n",
    "    print('Results for: ' + str(k) + '\\n')\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    print(classification_report(y_test, pred))\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(acc)\n",
    "    print('\\n' + '\\n')\n",
    "    keys.append(k)\n",
    "    scores.append(acc)\n",
    "    f1_scores.append(f1_score(y_test,pred))\n",
    "    y_prob = model.fit(X_train,y_train).predict_proba(X_test)[:,1]\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(str(model)+': Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "table = pd.DataFrame({'model':keys, 'accuracy score':scores,'F1 score':f1_scores,'time':times})\n",
    "table['threshold'] = pd.Series([threshold for i in range(len(scores))])\n",
    "table['features'] = pd.Series([len(features)-1 for i in range(len(scores))])\n",
    "tables.append(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>time</th>\n",
       "      <th>threshold</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105523</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025995</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.148302</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.573260</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.365889</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy score  F1 score       time  threshold  features\n",
       "0  Logistic Regression             1.0       1.0   0.105523          0       112\n",
       "1        Decision Tree             1.0       1.0   0.025995          0       112\n",
       "2        Random Forest             1.0       1.0   0.148302          0       112\n",
       "3  K-Nearest Neighbors             1.0       1.0   2.573260          0       112\n",
       "4           Linear SVM             1.0       1.0  10.365889          0       112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat([i for i in tables],axis=0)\n",
    "results.to_csv('./output_data/model_selection_results.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corr[np.abs(df_corr['class'])>0.540025]['class'].index)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n",
      "7\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for t in [.50,0.51,.54,0.5401,0.54002,0.540025,.55]:\n",
    "    print(len(df_corr[np.abs(df_corr['class'])>t]['class'].index)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,roc_curve,auc\n",
    "\n",
    "def model_performance(X,y):\n",
    "    models = {'Logistic Regression': LogisticRegression(class_weight='balanced'), 'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "          'Random Forest': RandomForestClassifier(n_estimators=30,class_weight='balanced'), \n",
    "          'K-Nearest Neighbors':KNeighborsClassifier(n_neighbors=1),\n",
    "            'Linear SVM':SVC(kernel='rbf', gamma=.10, C=1.0,probability=True)}\n",
    "    times =[]\n",
    "    keys = []\n",
    "    scores = []\n",
    "    f1_scores = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "    for k,v in models.items():\n",
    "        model = v\n",
    "        t0=time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time()-t0\n",
    "        t1 = time.time()\n",
    "        pred = model.predict(X_test)\n",
    "        predict_time = time.time()-t1\n",
    "        Time_total = train_time+predict_time\n",
    "        times.append(Time_total)\n",
    "        print('Results for: ' + str(k) + '\\n')\n",
    "        print(confusion_matrix(y_test, pred))\n",
    "        print(classification_report(y_test, pred))\n",
    "        acc = accuracy_score(y_test, pred)        \n",
    "        keys.append(k)\n",
    "        scores.append(acc)\n",
    "        f1_scores.append(f1_score(y_test,pred))\n",
    "        y_prob = model.fit(X_train,y_train).predict_proba(X_test)[:,1]\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)        \n",
    "    table = pd.DataFrame({'model':keys, 'accuracy score':scores,'F1 score':f1_scores,'time':times})\n",
    "    table['features'] = pd.Series([len(features) for i in range(len(scores))])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Logistic Regression\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Decision Tree\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: Random Forest\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: K-Nearest Neighbors\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n",
      "Results for: Linear SVM\n",
      "\n",
      "[[1274    0]\n",
      " [   0 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1274\n",
      "           1       1.00      1.00      1.00      1164\n",
      "\n",
      "    accuracy                           1.00      2438\n",
      "   macro avg       1.00      1.00      1.00      2438\n",
      "weighted avg       1.00      1.00      1.00      2438\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>time</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094814</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.191495</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.154204</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  accuracy score  F1 score       time  features\n",
       "0   Logistic Regression             1.0       1.0   0.048277         1\n",
       "1         Decision Tree             1.0       1.0   0.007636         1\n",
       "2         Random Forest             1.0       1.0   0.149254         1\n",
       "3   K-Nearest Neighbors             1.0       1.0   0.130554         1\n",
       "4            Linear SVM             1.0       1.0   0.030291         1\n",
       "..                  ...             ...       ...        ...       ...\n",
       "0   Logistic Regression             1.0       1.0   0.061614       113\n",
       "1         Decision Tree             1.0       1.0   0.012762       113\n",
       "2         Random Forest             1.0       1.0   0.094814       113\n",
       "3   K-Nearest Neighbors             1.0       1.0   1.191495       113\n",
       "4            Linear SVM             1.0       1.0  10.154204       113\n",
       "\n",
       "[565 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables1 = []\n",
    "for index in corr.index:\n",
    "    features = list(corr.loc[:index]['Feature'])\n",
    "    X = df[features]\n",
    "    y = df['class']\n",
    "    tables1.append(model_performance(X,y))\n",
    "tables11 = pd.concat([i for i in tables1],axis=0)\n",
    "tables11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables11.to_csv('./output_data/final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>time</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094814</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.191495</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.154204</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  accuracy score  F1 score       time  features\n",
       "0   Logistic Regression             1.0       1.0   0.048277         1\n",
       "1         Decision Tree             1.0       1.0   0.007636         1\n",
       "2         Random Forest             1.0       1.0   0.149254         1\n",
       "3   K-Nearest Neighbors             1.0       1.0   0.130554         1\n",
       "4            Linear SVM             1.0       1.0   0.030291         1\n",
       "..                  ...             ...       ...        ...       ...\n",
       "0   Logistic Regression             1.0       1.0   0.061614       113\n",
       "1         Decision Tree             1.0       1.0   0.012762       113\n",
       "2         Random Forest             1.0       1.0   0.094814       113\n",
       "3   K-Nearest Neighbors             1.0       1.0   1.191495       113\n",
       "4            Linear SVM             1.0       1.0  10.154204       113\n",
       "\n",
       "[565 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebuild",
   "language": "python",
   "name": "ebuild"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
